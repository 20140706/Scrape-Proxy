#!/usr/bin/env python3
"""
GitHub Actions SOCKS5ä»£ç†æµ‹è¯•å·¥å…·
è‡ªåŠ¨ä»å¤šä¸ªæ¥æºè·å–ä»£ç†ï¼Œæµ‹è¯•å¯ç”¨æ€§ï¼Œå¹¶ä¿å­˜ç»“æœ
"""

import requests
import random
import os
import sys
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import logging
from typing import List, Dict, Optional, Tuple

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('proxy_test.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ä»£ç†æ¥æº
PROXY_SOURCES = [
    "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/socks5.txt",
    "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies_anonymous/socks5.txt",
    "https://raw.githubusercontent.com/ErcinDedeoglu/proxies/main/proxies/socks5.txt",
    "https://raw.githubusercontent.com/Zaeem20/FREE_PROXIES_LIST/master/socks5.txt",
    "https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/socks5.txt",
    "https://raw.githubusercontent.com/hookzof/socks5_list/master/proxy.txt",
    "https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/socks5.txt"
]

# æµ‹è¯•ç½‘ç«™
TEST_WEBSITES = [
    "https://icanhazip.com",  # è¿”å›IP
    "https://api.ipify.org",   # è¿”å›IP
    "http://httpbin.org/ip",   # è¿”å›JSONæ ¼å¼IP
]

# User-Agentåˆ—è¡¨
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

class ProxyTester:
    """SOCKS5ä»£ç†æµ‹è¯•å™¨"""
    
    def __init__(self, max_workers: int = 20, timeout: int = 10):
        """
        åˆå§‹åŒ–ä»£ç†æµ‹è¯•å™¨
        
        Args:
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_workers = max_workers
        self.timeout = timeout
        self.session = requests.Session()
        self.results = []
        self.working_proxies = []
        
    def get_proxy_sources(self) -> List[str]:
        """è·å–ä»£ç†æ¥æºåˆ—è¡¨"""
        return PROXY_SOURCES
    
    def fetch_proxies_from_source(self, url: str) -> List[str]:
        """ä»å•ä¸ªæ¥æºè·å–ä»£ç†"""
        try:
            headers = {'User-Agent': random.choice(USER_AGENTS)}
            response = self.session.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            
            # è§£æä»£ç†åˆ—è¡¨
            proxies = []
            for line in response.text.splitlines():
                line = line.strip()
                if line and not line.startswith('#'):
                    # æ¸…ç†ä»£ç†æ ¼å¼
                    proxy = line.split()[0] if ' ' in line else line
                    if ':' in proxy:
                        proxies.append(proxy)
            
            logger.info(f"ä» {url} è·å–åˆ° {len(proxies)} ä¸ªä»£ç†")
            return proxies
        except Exception as e:
            logger.warning(f"ä» {url} è·å–ä»£ç†å¤±è´¥: {str(e)}")
            return []
    
    def fetch_all_proxies(self) -> List[str]:
        """ä»æ‰€æœ‰æ¥æºè·å–ä»£ç†"""
        all_proxies = []
        
        logger.info(f"å¼€å§‹ä» {len(PROXY_SOURCES)} ä¸ªæ¥æºè·å–ä»£ç†...")
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {executor.submit(self.fetch_proxies_from_source, url): url 
                      for url in self.get_proxy_sources()}
            
            for future in as_completed(futures):
                url = futures[future]
                try:
                    proxies = future.result()
                    all_proxies.extend(proxies)
                except Exception as e:
                    logger.error(f"å¤„ç† {url} æ—¶å‡ºé”™: {str(e)}")
        
        # å»é‡
        unique_proxies = list(set(all_proxies))
        logger.info(f"è·å–åˆ° {len(unique_proxies)} ä¸ªå”¯ä¸€ä»£ç†")
        
        return unique_proxies
    
    def test_single_proxy(self, proxy: str, website: str) -> Optional[Dict]:
        """æµ‹è¯•å•ä¸ªä»£ç†åœ¨å•ä¸ªç½‘ç«™ä¸Šçš„å¯ç”¨æ€§"""
        try:
            # è§£æä»£ç†
            if '://' in proxy:
                proxy_url = proxy
            else:
                proxy_url = f"socks5://{proxy}"
            
            # å‡†å¤‡è¯·æ±‚
            headers = {
                'User-Agent': random.choice(USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            proxies = {
                'http': proxy_url,
                'https': proxy_url
            }
            
            # å‘é€è¯·æ±‚
            start_time = time.time()
            response = self.session.get(
                website, 
                proxies=proxies, 
                headers=headers, 
                timeout=self.timeout,
                allow_redirects=True
            )
            latency = time.time() - start_time
            
            response.raise_for_status()
            
            # è§£æå“åº”
            if response.status_code == 200:
                # è·å–è¿”å›çš„IP
                if 'json' in response.headers.get('Content-Type', ''):
                    ip_data = response.json()
                    if 'ip' in ip_data:
                        returned_ip = ip_data['ip']
                    elif 'origin' in ip_data:
                        returned_ip = ip_data['origin']
                    else:
                        returned_ip = response.text.strip()
                else:
                    returned_ip = response.text.strip()
                
                return {
                    'proxy': proxy,
                    'website': website,
                    'status_code': response.status_code,
                    'ip': returned_ip,
                    'latency': round(latency, 2),
                    'success': True
                }
            
        except requests.exceptions.Timeout:
            logger.debug(f"ä»£ç† {proxy} åœ¨ {website} ä¸Šè¶…æ—¶")
        except requests.exceptions.ProxyError as e:
            logger.debug(f"ä»£ç† {proxy} è¿æ¥å¤±è´¥: {str(e)}")
        except requests.exceptions.ConnectionError as e:
            logger.debug(f"ä»£ç† {proxy} è¿æ¥é”™è¯¯: {str(e)}")
        except requests.exceptions.RequestException as e:
            logger.debug(f"ä»£ç† {proxy} è¯·æ±‚å¼‚å¸¸: {str(e)}")
        except Exception as e:
            logger.debug(f"æµ‹è¯•ä»£ç† {proxy} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        
        return None
    
    def test_proxy_on_all_sites(self, proxy: str) -> List[Dict]:
        """æµ‹è¯•ä»£ç†åœ¨æ‰€æœ‰ç½‘ç«™ä¸Šçš„è¡¨ç°"""
        proxy_results = []
        
        for website in TEST_WEBSITES:
            result = self.test_single_proxy(proxy, website)
            if result:
                proxy_results.append(result)
            else:
                # ä»»æ„ä¸€ä¸ªç½‘ç«™å¤±è´¥ï¼Œåˆ™è®¤ä¸ºä»£ç†ä¸å¯ç”¨
                return []
        
        return proxy_results
    
    def test_proxies_batch(self, proxies: List[str], max_tests: int = 50) -> Tuple[List[Dict], List[str]]:
        """
        æ‰¹é‡æµ‹è¯•ä»£ç†
        
        Args:
            proxies: ä»£ç†åˆ—è¡¨
            max_tests: æœ€å¤§æµ‹è¯•æ•°é‡
            
        Returns:
            Tuple[æµ‹è¯•ç»“æœåˆ—è¡¨, å¯ç”¨ä»£ç†åˆ—è¡¨]
        """
        all_results = []
        working_proxies = []
        
        # é™åˆ¶æµ‹è¯•æ•°é‡
        test_proxies = proxies[:max_tests] if len(proxies) > max_tests else proxies
        logger.info(f"å¼€å§‹æµ‹è¯• {len(test_proxies)} ä¸ªä»£ç†...")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_proxy = {executor.submit(self.test_proxy_on_all_sites, proxy): proxy 
                              for proxy in test_proxies}
            
            for i, future in enumerate(as_completed(future_to_proxy), 1):
                proxy = future_to_proxy[future]
                
                if i % 10 == 0:
                    logger.info(f"å·²æµ‹è¯• {i}/{len(test_proxies)} ä¸ªä»£ç†ï¼Œæ‰¾åˆ° {len(working_proxies)} ä¸ªå¯ç”¨ä»£ç†")
                
                try:
                    results = future.result(timeout=self.timeout + 5)
                    if results:
                        all_results.extend(results)
                        working_proxies.append(proxy)
                        logger.info(f"âœ“ ä»£ç† {proxy} å¯ç”¨ (å»¶è¿Ÿ: {results[0]['latency']}ç§’)")
                except Exception as e:
                    logger.debug(f"æµ‹è¯•ä»£ç† {proxy} æ—¶å‡ºé”™: {str(e)}")
        
        logger.info(f"æµ‹è¯•å®Œæˆã€‚å…±æ‰¾åˆ° {len(working_proxies)} ä¸ªå¯ç”¨ä»£ç†")
        return all_results, working_proxies
    
    def save_results(self, results: List[Dict], working_proxies: List[str], 
                    all_proxies_count: int) -> None:
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        
        # 1. ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶
        detailed_results = {
            'test_time': datetime.now().isoformat(),
            'total_proxies_fetched': all_proxies_count,
            'total_proxies_tested': len(set(r['proxy'] for r in results)),
            'working_proxies_count': len(working_proxies),
            'test_websites': TEST_WEBSITES,
            'results': results
        }
        
        with open('proxy_results.json', 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        # 2. ä¿å­˜å¯ç”¨çš„ä»£ç†åˆ—è¡¨åˆ°æ–‡æœ¬æ–‡ä»¶
        with open('available_proxies.txt', 'w', encoding='utf-8') as f:
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# æ€»ä»£ç†æ•°: {all_proxies_count}\n")
            f.write(f"# å¯ç”¨ä»£ç†æ•°: {len(working_proxies)}\n")
            f.write("# æ ¼å¼: IP:ç«¯å£\n")
            f.write("\n".join(working_proxies))
        
        # 3. ä¿å­˜å•ä¸ªæœ€ä½³ä»£ç†ï¼ˆå»¶è¿Ÿæœ€ä½çš„ï¼‰
        if working_proxies and results:
            # æŒ‰å»¶è¿Ÿæ’åº
            sorted_results = sorted(results, key=lambda x: x['latency'])
            best_proxy = sorted_results[0]['proxy']
            
            with open('BEST_SOCKS5.txt', 'w', encoding='utf-8') as f:
                f.write(best_proxy)
            
            logger.info(f"æœ€ä½³ä»£ç†å·²ä¿å­˜: {best_proxy}")
        
        # 4. ç”ŸæˆHTMLæŠ¥å‘Š
        self.generate_html_report(detailed_results)
    
    def generate_html_report(self, data: Dict) -> None:
        """ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š"""
        html = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>SOCKS5ä»£ç†æµ‹è¯•æŠ¥å‘Š</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
                .summary {{ background: #f5f5f5; padding: 20px; border-radius: 5px; margin-bottom: 20px; }}
                .summary h2 {{ margin-top: 0; }}
                .stat {{ display: inline-block; margin-right: 20px; background: white; padding: 10px; border-radius: 3px; }}
                .proxy-list {{ background: #e8f4f8; padding: 15px; border-radius: 5px; }}
                .proxy-item {{ 
                    background: white; 
                    margin: 5px 0; 
                    padding: 10px; 
                    border-left: 4px solid #4CAF50;
                    border-radius: 3px;
                }}
                .latency {{ color: #666; font-size: 0.9em; }}
                .good {{ color: #4CAF50; }}
                .medium {{ color: #FF9800; }}
                .poor {{ color: #F44336; }}
                table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <h1>ğŸ“¡ SOCKS5ä»£ç†æµ‹è¯•æŠ¥å‘Š</h1>
            
            <div class="summary">
                <h2>ğŸ“Š æµ‹è¯•æ¦‚è§ˆ</h2>
                <div class="stat">æ€»è·å–ä»£ç†æ•°: {data['total_proxies_fetched']}</div>
                <div class="stat">æµ‹è¯•ä»£ç†æ•°: {data['total_proxies_tested']}</div>
                <div class="stat">å¯ç”¨ä»£ç†æ•°: <span class="good">{data['working_proxies_count']}</span></div>
                <div class="stat">æµ‹è¯•æ—¶é—´: {data['test_time']}</div>
            </div>
            
            <h2>âœ… å¯ç”¨ä»£ç†åˆ—è¡¨</h2>
        """
        
        if data['results']:
            # æŒ‰ä»£ç†åˆ†ç»„
            proxy_groups = {}
            for result in data['results']:
                proxy = result['proxy']
                if proxy not in proxy_groups:
                    proxy_groups[proxy] = []
                proxy_groups[proxy].append(result)
            
            # è®¡ç®—æ¯ä¸ªä»£ç†çš„å¹³å‡å»¶è¿Ÿ
            proxy_stats = []
            for proxy, results in proxy_groups.items():
                avg_latency = sum(r['latency'] for r in results) / len(results)
                proxy_stats.append({
                    'proxy': proxy,
                    'avg_latency': avg_latency,
                    'success_rate': 100 * len(results) / len(TEST_WEBSITES)
                })
            
            # æŒ‰å»¶è¿Ÿæ’åº
            proxy_stats.sort(key=lambda x: x['avg_latency'])
            
            html += "<table>"
            html += "<tr><th>æ’å</th><th>ä»£ç†</th><th>å¹³å‡å»¶è¿Ÿ(ç§’)</th><th>æˆåŠŸç‡</th><th>çŠ¶æ€</th></tr>"
            
            for i, stat in enumerate(proxy_stats, 1):
                latency_class = "good" if stat['avg_latency'] < 2 else "medium" if stat['avg_latency'] < 5 else "poor"
                html += f"""
                <tr>
                    <td>#{i}</td>
                    <td>{stat['proxy']}</td>
                    <td class="{latency_class}">{stat['avg_latency']:.2f}</td>
                    <td>{stat['success_rate']:.1f}%</td>
                    <td class="good">âœ“ å¯ç”¨</td>
                </tr>
                """
            
            html += "</table>"
            
            # æ·»åŠ å‰5ä¸ªä»£ç†çš„è¯¦ç»†ä¿¡æ¯
            html += "<h3>ğŸ† æœ€ä½³ä»£ç†è¯¦æƒ…</h3>"
            for i, stat in enumerate(proxy_stats[:5], 1):
                html += f"""
                <div class="proxy-item">
                    <strong>#{i}: {stat['proxy']}</strong>
                    <div class="latency">å¹³å‡å»¶è¿Ÿ: {stat['avg_latency']:.2f}ç§’</div>
                </div>
                """
        else:
            html += "<p>âŒ æœªæ‰¾åˆ°å¯ç”¨ä»£ç†</p>"
        
        html += f"""
            <hr>
            <footer>
                <p><small>æµ‹è¯•ç½‘ç«™: {', '.join(TEST_WEBSITES)}</small></p>
                <p><small>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</small></p>
            </footer>
        </body>
        </html>
        """
        
        with open('proxy_report.html', 'w', encoding='utf-8') as f:
            f.write(html)
    
    def load_previous_best_proxy(self) -> Optional[str]:
        """åŠ è½½ä¹‹å‰ä¿å­˜çš„æœ€ä½³ä»£ç†"""
        if os.path.exists('BEST_SOCKS5.txt'):
            try:
                with open('BEST_SOCKS5.txt', 'r') as f:
                    proxy = f.read().strip()
                    if proxy and ':' in proxy:
                        logger.info(f"åŠ è½½ä¹‹å‰çš„æœ€ä½³ä»£ç†: {proxy}")
                        return proxy
            except Exception as e:
                logger.warning(f"åŠ è½½ä¹‹å‰çš„æœ€ä½³ä»£ç†å¤±è´¥: {str(e)}")
        return None
    
    def run(self) -> None:
        """ä¸»è¿è¡Œå‡½æ•°"""
        logger.info("ğŸš€ å¼€å§‹SOCKS5ä»£ç†æµ‹è¯•")
        start_time = datetime.now()
        
        try:
            # 1. å°è¯•å…ˆæµ‹è¯•ä¹‹å‰çš„æœ€ä½³ä»£ç†
            previous_best = self.load_previous_best_proxy()
            if previous_best:
                logger.info(f"æµ‹è¯•ä¹‹å‰çš„æœ€ä½³ä»£ç†: {previous_best}")
                test_result = self.test_proxy_on_all_sites(previous_best)
                if test_result:
                    logger.info(f"âœ… ä¹‹å‰çš„æœ€ä½³ä»£ç†ä»ç„¶å¯ç”¨: {previous_best}")
                    self.working_proxies = [previous_best]
                    self.results = test_result
                    
                    # ä¿å­˜ç»“æœ
                    self.save_results(test_result, [previous_best], 1)
                    
                    end_time = datetime.now()
                    logger.info(f"âœ… æµ‹è¯•å®Œæˆ (è€—æ—¶: {(end_time - start_time).total_seconds():.1f}ç§’)")
                    logger.info(f"âœ… ä¹‹å‰çš„æœ€ä½³ä»£ç†ä»ç„¶å¯ç”¨ï¼Œæ— éœ€é‡æ–°æµ‹è¯•æ‰€æœ‰ä»£ç†")
                    return
            
            # 2. è·å–æ‰€æœ‰ä»£ç†
            all_proxies = self.fetch_all_proxies()
            
            if not all_proxies:
                logger.error("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•ä»£ç†")
                return
            
            # 3. éšæœºæ‰“ä¹±ä»£ç†åˆ—è¡¨
            random.shuffle(all_proxies)
            
            # 4. æ‰¹é‡æµ‹è¯•ä»£ç†
            results, working_proxies = self.test_proxies_batch(all_proxies, max_tests=100)
            
            # 5. ä¿å­˜ç»“æœ
            if results and working_proxies:
                self.save_results(results, working_proxies, len(all_proxies))
                
                # æ˜¾ç¤ºæœ€ä½³ä»£ç†
                best_proxy = min(results, key=lambda x: x['latency'])
                logger.info(f"ğŸ† æœ€ä½³ä»£ç†: {best_proxy['proxy']} (å»¶è¿Ÿ: {best_proxy['latency']}ç§’)")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨ä»£ç†")
                # ä¿å­˜ç©ºç»“æœ
                with open('available_proxies.txt', 'w', encoding='utf-8') as f:
                    f.write("# æœªæ‰¾åˆ°å¯ç”¨ä»£ç†\n")
            
            end_time = datetime.now()
            logger.info(f"âœ… æµ‹è¯•å®Œæˆ (è€—æ—¶: {(end_time - start_time).total_seconds():.1f}ç§’)")
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
            raise

def main():
    """ç¨‹åºå…¥å£ç‚¹"""
    try:
        # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
        tester = ProxyTester(max_workers=15, timeout=8)
        
        # è¿è¡Œæµ‹è¯•
        tester.run()
        
        # æ‰“å°æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ¯ SOCKS5ä»£ç†æµ‹è¯•æ€»ç»“")
        print("="*60)
        
        if os.path.exists('available_proxies.txt'):
            with open('available_proxies.txt', 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if len(lines) > 3:  # è·³è¿‡æ³¨é‡Šè¡Œ
                    print(f"âœ… æ‰¾åˆ° {len(lines)-3} ä¸ªå¯ç”¨ä»£ç†")
                    print(f"ğŸ“ ç»“æœæ–‡ä»¶: available_proxies.txt, proxy_results.json, proxy_report.html")
                    print(f"ğŸ† æœ€ä½³ä»£ç†: {open('BEST_SOCKS5.txt').read().strip() if os.path.exists('BEST_SOCKS5.txt') else 'æ— '}")
                else:
                    print("âŒ æœªæ‰¾åˆ°å¯ç”¨ä»£ç†")
        
        print("="*60)
        
    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
