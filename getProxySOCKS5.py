#!/usr/bin/env python3
"""
GitHub Actions SOCKS5ä»£ç†æµ‹è¯•å·¥å…· - ä¿®å¤ç‰ˆ
"""

import requests
import random
import os
import sys
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('proxy_test.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ä»£ç†æ¥æº
PROXY_SOURCES = [
    "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/socks5.txt",
    "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies_anonymous/socks5.txt",
    "https://raw.githubusercontent.com/ErcinDedeoglu/proxies/main/proxies/socks5.txt",
    "https://raw.githubusercontent.com/Zaeem20/FREE_PROXIES_LIST/master/socks5.txt",
]

# æµ‹è¯•ç½‘ç«™
TEST_WEBSITES = [
    "https://icanhazip.com",
    "https://api.ipify.org",
]

class ProxyTester:
    def __init__(self, max_workers=10, timeout=8):
        self.max_workers = max_workers
        self.timeout = timeout
        self.session = requests.Session()
        
    def fetch_proxies(self):
        """è·å–ä»£ç†åˆ—è¡¨"""
        all_proxies = []
        for url in PROXY_SOURCES:
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    proxies = [line.strip() for line in response.text.split('\n') 
                              if line.strip() and ':' in line and not line.startswith('#')]
                    all_proxies.extend(proxies)
                    logger.info(f"ä» {url} è·å–åˆ° {len(proxies)} ä¸ªä»£ç†")
            except Exception as e:
                logger.warning(f"è·å– {url} å¤±è´¥: {e}")
        
        return list(set(all_proxies))
    
    def test_proxy(self, proxy):
        """æµ‹è¯•å•ä¸ªä»£ç†"""
        try:
            proxy_url = f"socks5://{proxy}"
            proxies = {'http': proxy_url, 'https': proxy_url}
            
            results = []
            for website in TEST_WEBSITES:
                start_time = time.time()
                response = self.session.get(website, proxies=proxies, timeout=self.timeout)
                latency = time.time() - start_time
                
                if response.status_code == 200:
                    results.append({
                        'proxy': proxy,
                        'website': website,
                        'status_code': response.status_code,
                        'ip': response.text.strip(),
                        'latency': round(latency, 2),
                        'success': True
                    })
            
            return results if len(results) == len(TEST_WEBSITES) else None
            
        except Exception as e:
            return None
    
    def test_proxies(self, proxies, max_tests=50):
        """æ‰¹é‡æµ‹è¯•ä»£ç†"""
        working_proxies = []
        results = []
        
        test_proxies = proxies[:max_tests]
        logger.info(f"å¼€å§‹æµ‹è¯• {len(test_proxies)} ä¸ªä»£ç†")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_proxy = {executor.submit(self.test_proxy, proxy): proxy 
                              for proxy in test_proxies}
            
            for future in as_completed(future_to_proxy):
                proxy = future_to_proxy[future]
                try:
                    proxy_results = future.result()
                    if proxy_results:
                        working_proxies.append(proxy)
                        results.extend(proxy_results)
                        logger.info(f"âœ“ {proxy} å¯ç”¨")
                except Exception:
                    pass
        
        return results, working_proxies
    
    def save_results(self, results, working_proxies, total_count):
        """ä¿å­˜ç»“æœ"""
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open('proxy_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                'test_time': datetime.now().isoformat(),
                'total_proxies': total_count,
                'working_proxies': len(working_proxies),
                'results': results
            }, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜å¯ç”¨ä»£ç†åˆ—è¡¨
        with open('available_proxies.txt', 'w', encoding='utf-8') as f:
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now()}\n")
            f.write(f"# æ€»ä»£ç†æ•°: {total_count}\n")
            f.write(f"# å¯ç”¨ä»£ç†æ•°: {len(working_proxies)}\n\n")
            f.write('\n'.join(working_proxies))
        
        # ä¿å­˜æœ€ä½³ä»£ç†
        if results:
            best = min(results, key=lambda x: x['latency'])
            with open('BEST_SOCKS5.txt', 'w') as f:
                f.write(best['proxy'])
    
    def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        logger.info("ğŸš€ å¼€å§‹ä»£ç†æµ‹è¯•")
        
        # è·å–ä»£ç†
        proxies = self.fetch_proxies()
        if not proxies:
            logger.error("âŒ æœªè·å–åˆ°ä»£ç†")
            return
        
        logger.info(f"ğŸ“Š è·å–åˆ° {len(proxies)} ä¸ªä»£ç†")
        
        # æµ‹è¯•ä»£ç†
        results, working_proxies = self.test_proxies(proxies)
        
        # ä¿å­˜ç»“æœ
        if working_proxies:
            self.save_results(results, working_proxies, len(proxies))
            logger.info(f"âœ… æµ‹è¯•å®Œæˆï¼Œæ‰¾åˆ° {len(working_proxies)} ä¸ªå¯ç”¨ä»£ç†")
            
            # ç”Ÿæˆç®€å•çš„HTMLæŠ¥å‘Š
            self.generate_html_report(results, len(proxies))
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨ä»£ç†")
    
    def generate_html_report(self, results, total_count):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        working_count = len(set(r['proxy'] for r in results))
        
        html = f"""
        <html>
        <head><title>ä»£ç†æµ‹è¯•æŠ¥å‘Š</title></head>
        <body>
            <h1>ä»£ç†æµ‹è¯•æŠ¥å‘Š</h1>
            <p>æµ‹è¯•æ—¶é—´: {datetime.now()}</p>
            <p>æ€»ä»£ç†æ•°: {total_count}</p>
            <p>å¯ç”¨ä»£ç†æ•°: {working_count}</p>
            <h2>å¯ç”¨ä»£ç†åˆ—è¡¨</h2>
            <ul>
        """
        
        for proxy in set(r['proxy'] for r in results):
            proxy_results = [r for r in results if r['proxy'] == proxy]
            avg_latency = sum(r['latency'] for r in proxy_results) / len(proxy_results)
            html += f'<li>{proxy} (å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}s)</li>'
        
        html += "</ul></body></html>"
        
        with open('proxy_report.html', 'w', encoding='utf-8') as f:
            f.write(html)

def main():
    tester = ProxyTester()
    tester.run()

if __name__ == "__main__":
    main()
